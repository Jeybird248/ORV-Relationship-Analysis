{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install spacy\n",
    "!pip install networkx\n",
    "!pip install matplotlib.pyplot\n",
    "!pip install re\n",
    "!pip install os\n",
    "!pip install pyvis\n",
    "!pip install python_louvain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyvis.network import Network\n",
    "from community import community_louvain\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "# installing and importing libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions\n",
    "\n",
    "def load_books(book_folder):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(book_folder):\n",
    "        file_path = os.path.join(book_folder, file_name)\n",
    "    \n",
    "        # Check if the path is a file (not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            texts.append(file_path)\n",
    "    return texts\n",
    "\n",
    "def filter_entity(ent_list, character_df):\n",
    "    return [ent for ent in ent_list\n",
    "            if ent in list(character_df.character)\n",
    "            or ent in list(character_df.character_firstname)\n",
    "            or any(ent in alias_list for alias_list in character_df.aliases)]\n",
    "\n",
    "def extract_last_name(full_name, last_names):\n",
    "    for last_name in last_names:\n",
    "        if last_name in full_name[0]:\n",
    "            return last_name\n",
    "    return full_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in text and langauge model\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "texts = load_books(\"book_split_output\")\n",
    "# for text in texts:\n",
    "#     book_doc = NER(open(text, 'r', encoding='utf-8').read())\n",
    "book_doc = NER(open(texts[0], 'r', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in names of characters\n",
    "character_df = pd.read_csv(\"names.csv\")\n",
    "\n",
    "\n",
    "# add columns for different variations of name\n",
    "character_df[\"character\"] = character_df[\"character\"].apply(lambda x: re.sub(r\"[^\\w\\s]+\", \"\", x))\n",
    "character_df[\"character_firstname\"] = character_df[\"character\"].apply(lambda x: x.split(\" \", 1) [0])\n",
    "character_df[\"character_lastname\"] = character_df[\"character\"].apply(lambda x: x.split(\" \", 1) [-1])\n",
    "character_df[\"aliases\"] = character_df[\"aliases\"].apply(lambda x: [] if isinstance(x, float) else x.split(\",\"))\n",
    "\n",
    "# use NER to process and grab the list of entities and their sentences\n",
    "sent_entity_df = []\n",
    "\n",
    "for sent in book_doc.sents:\n",
    "    entity_list = [ent.text for ent in sent.ents]\n",
    "    sent_entity_df.append({\"sentence\": sent, \"entities\": entity_list})\n",
    "\n",
    "# turn list into df\n",
    "sent_entity_df = pd.DataFrame(sent_entity_df)\n",
    "\n",
    "# filter out only the character names\n",
    "sent_entity_df[\"character_entities\"] = sent_entity_df[\"entities\"].apply(lambda x: filter_entity(x, character_df))\n",
    "\n",
    "# filter out sentences that don't have any entities in them\n",
    "sent_entity_df_filtered = sent_entity_df[sent_entity_df[\"character_entities\"].map(len) > 0]\n",
    "\n",
    "# set entity name to last name of entity to aggregate same names i.e. Kim Dokja vs Dokja should be counted as same\n",
    "sent_entity_df_filtered['character_entities'] = sent_entity_df_filtered.apply(lambda row: [extract_last_name(row['character_entities'], character_df['character_lastname'])], axis=1)\n",
    "pd.reset_option('^display.', silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scroll through 5 lines at a time to aggregate relationships between characters and reorder to make sure A -> B is the same as B -> A\n",
    "window_size = 5\n",
    "relationships = []\n",
    "\n",
    "for i in range(sent_entity_df_filtered.index[-1]):\n",
    "    end_i = min(i+5, sent_entity_df_filtered.index[-1])\n",
    "    char_list = sum((sent_entity_df_filtered.loc[i : end_i].character_entities), [])\n",
    "\n",
    "    char_unique = [char_list[i] for i in range(len(char_list))\n",
    "                   if (i == 0) or char_list[i] != char_list[i-1]]\n",
    "    if len(char_unique) > 1:\n",
    "        for idx, a in enumerate(char_unique[:-1]):\n",
    "            b = char_unique[idx + 1]\n",
    "            relationships.append({\"source\": a, \"target\": b})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurences of relationships between characters\n",
    "relationship_df = pd.DataFrame(relationships)\n",
    "relationship_df.head(10)\n",
    "relationship_df = pd.DataFrame(np.sort(relationship_df.values, axis = 1), columns = relationship_df.columns)\n",
    "\n",
    "relationship_df[\"value\"] = 1\n",
    "relationship_df = relationship_df.groupby([\"source\", \"target\"], sort=False, as_index = False). sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(relationship_df,\n",
    "                           source = \"source\",\n",
    "                           target = \"target\",\n",
    "                           edge_attr = \"value\",\n",
    "                           create_using = nx.Graph())\n",
    "#this sets up our network for later graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color=\"skyblue\", edge_cmap=plt.cm.Blues, pos=pos)\n",
    "plt.show()\n",
    "\n",
    "#this is the bad graph that's too clustered to see anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = Network(notebook = True, width=\"1000px\", height=\"700px\", bgcolor='#222222',font_color='white', cdn_resources='in_line')\n",
    "node_degree = dict(G.degree)\n",
    "nx.set_node_attributes(G, node_degree, \"size\")\n",
    "net.from_nx(G)\n",
    "net.show(\"orv.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(G.degree)\n",
    "#just to see how many connections there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree centrality\n",
    "degree_dict = nx.degree_centrality(G)\n",
    "degree_df = pd.DataFrame.from_dict(centrality_dict, orient='index', columns=['centrality'])\n",
    "degree_df.sort_values('centrality', ascending=False)[0:9].plot(kind=\"bar\")\n",
    "#centrality graph of first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#betweeness centrality \n",
    "betweenness_dict = nx.betweenness_centrality(G)\n",
    "betweenness_df = pd.DataFrame.from_dict(betweenness_dict, orient='index', columns=['centrality'])\n",
    "betweenness_df.sort_values('centrality', ascending=False)[0:9].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closeness centrality\n",
    "closeness_dict = nx.closeness_centrality(G)\n",
    "closeness_df = pd.DataFrame.from_dict(closeness_dict, orient='index', columns=['centrality'])\n",
    "closeness_df.sort_values('centrality', ascending=False)[0:9].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving all centrality measures\n",
    "nx.set_node_attributes(G, degree_dict, 'degree_centrality')\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness_centrality')\n",
    "nx.set_node_attributes(G, closeness_dict, 'closeness_centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting people into which attributes best\n",
    "communities = community_louvain.best_partition(G)\n",
    "communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the communities for the graph\n",
    "nx.set_node_attributes(G, communities, \"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#color time\n",
    "com_net = Network(notebook = True, width=\"1000px\", height=\"700px\", bgcolor='#222222',font_color='white')\n",
    "com_net.from_nx(G)\n",
    "com_net.show(\"orv_communities.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
